<!DOCTYPE HTML>
<!--
	Brian Johnson Robotics
	Adapted from a template from html5up.net | @ajlkn (html5up.net/license)
-->
<html>
	<head>
		<title>Brian Johnson Robotics</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<link rel="stylesheet" href="../assets/css/prism.css" />
	</head>
	<body class="is-preload no-sidebar">
		<script src="../assets/js/prism.js"></script>
		<div id="page-wrapper">

			<!-- Header -->
			<div id="header-wrapper">
				<header id="header" class="container">

					<!-- Logo -->
						<div id="logo">
							<h1><a href="../index.html">Brian Johnson, Ph.D.</a></h1>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="../index.html">Home</a></li>
								<li class="current"><a href="../projects.html">Projects</a></li>
								<li><a href="../publications.html">Publications</a></li>
								<li><a href="../resume.html">Resume and Contact</a></li>
							</ul>
						</nav>

				</header>
			</div>

			<!-- Main -->
				<div id="main-wrapper">
					<div class="container blog">
						<div id="content">

							<!-- Content -->
								<article>
									<div class="container">
											<a class="image featured"><img src="../images/stock_chinese.jpg" style="display:block; height:40%; width:30%; position:relative; margin-left: auto; margin-right: auto;"/></a>
									</div>
									
									
									<h2>Chinese Text Simplification for Reading Comprehension (Part 3)</h2>

									<div class="container" style="text-align: center; margin-top:-1em; margin-bottom:2em; font-weight:bold; font-size:18px;">
										<div class="row">
											<div class="col-6 col-12-small">
												<a href="../Projects/textsimplification_part2.html">(Back to Part 2)</a>
											</div>
											<div class="col-6 col-12-small">
												<a href="../Projects/textsimplification_part4.html">(To Part 4)</a>
											</div>	
										</div>

									</div>

									<h3>Part 3 Summary</h3>
									<p><i>I describe my process of fine-tuning and applying a Chinese language BART model to 
										achieve generalized text simplification.
									</i></p>

									<h3>Fine-tuning a BART model for sentence-level text simplification</h3>
									<p>
										<b>Bidirectional and Auto-Regressive Transformer (BART)</b> is a sequence-to-sequence model proposed by Lewis et al. <a href="#lewis2019">[1]</a> which uses 
										an encoder and decoder for NLP tasks. For some in-depth explanations, check out these
										resources for <a href="https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b" target="_blank">sequence-to-sequence models</a>, 
										<a href="https://huggingface.co/blog/bert-101" target="_blank">BERT</a>, and 
										<a href="https://www.analyticsvidhya.com/blog/2024/11/bart-model/" target="_blank">BART</a>.
										I will be fine-tuning an existing BART model for a sequence generation task: generating
										a simplified Chinese sentence based on a complex sentence input!
									</p>

									<div class="container">
										<a class="image centered"><img src="../images/project_TS/bart_lewis_2019.PNG" style="width:60%;"/></a>
										<figcaption>BART model as explained in Lewis et al. <a href="#lewis2019">[1]</a></figcation>
									</div>

									<p>
										Fine-tuning BART for text simplification tasks has been demonstrated in English <a href="#sun2023">[2]</a> as well as
										Chinese <a href="#chong2023">[3]</a> and has shown performance better than large language models like GPT 3.5
										when properly fine-tuned. I will be using a BART model trained on Chinese language data, <b>CPT/Chinese BART</b>,
										available on HuggingFace: <a href="https://huggingface.co/fnlp/bart-base-chinese", target="_blank">https://huggingface.co/fnlp/bart-base-chinese</a>.
										The model was implemented from the architecture described in Shao et al. <a href="#shao2021">[4]</a>
									</p>

									<p>Chinese BART is trained from masking sequences to predict the masked word; it is not trained for text simplification 
										and will need to be fine-tuned. Let's get into how to do that!
									</p>

									<h4>Data for fine-tuning</h4>
									<p>
										The concept of fine-tuning is simple enough -- you use the pre-trained model with its preexisting model weights and biases,
										and continue training the model from its current state using task-specific data. In this case, I want to train Chinese BART
										with text simplification parallel data - a complex sentence input, with a simple sentence to be predicted as the output.
									</p>
									<p>
										As I discussed in <a href="../Projects/textsimplification.html">part 1</a> of this project, the 
										MCTS paper <a href="#chong2024">[3]</a> provides a dataset of 691,474 pseudo-simplified sentence pairs, 
										generated by machine-translating complex Chinese sentences from the People's Daily Corpus to English, 
										applying English TS tools, and retranslating the simplified sentences back to Chinese.  								
									</p>
									<p>
										Since this dataset is "pseudo" data coming from machine translation, the fluency of the output is unfortunately
										not guaranteed. However, at the time of writing there is a significant lack of datasets for Chinese language TS
										compared to other languages like English, so the MCTS dataset is one of the best options available.
									</p>

									<h4>Fine-tuning Chinese BART</h4>
									<p>Since our BART model is prepared on HuggingFace, we can use the HuggingFace transformers
										package and framework for our fine-tuning. Let's start by preparing our data. Rather than 
										input the MCTS data directly into the tokenizer, I want to add tag the sentence pairs with
										some custom tokens for HSK vocabulary.
									</p>

									<!--TODO: add discussion of HSK distribution here-->
									
									<p>Let's start by importing some base packages, loading our model, tokenizer, and
										the HSK vocabulary dataset that I processed in Part 2:
									</p>
									<pre class="language-py"><code class="language-py">
import numpy as np
import jieba
import pickle
from transformers import TrainingArguments, Trainer
from datasets import load_dataset
from datasets import Dataset
from transformers import BertTokenizer, BartForConditionalGeneration
tokenizer = BertTokenizer.from_pretrained("fnlp/bart-base-chinese")
model = BartForConditionalGeneration.from_pretrained("fnlp/bart-base-chinese")

with open("HSK_levels.pickle", 'rb') as handle:
	HSK_dict = pickle.load(handle)
									</code></pre>

									<p>
										Now define our preprocessing function to add HSK tokens for HSK levels 4, 5, 6, and 7-9:
									</p>

									<pre class="language-py"><code class="language-py">	
def tokenize_with_HSK(sentence, HSK_dict):
    split_sentence = jieba.lcut(sentence)
    HSK_sentence = ""
    for word in split_sentence:
        score = HSK_dict.get(word, 0)
        if score>3:
            HSK_sentence += f"{word}[{score}]"
        else:
            HSK_sentence += f"{word}"
    return HSK_sentence

tokenizer.add_tokens(["[4]", "[5]", "[6]", "[7]"])
model.resize_token_embeddings(len(tokenizer))

def preprocess_data(filename: str, start: int, stop: int):
    lines_HSK = []
    with open(filename, encoding="utf8") as f:
        lines_orig = f.read().splitlines()
        for line in lines_orig[start:stop]:
            lines_HSK.append(tokenize_with_HSK(line, HSK_dict))
    return lines_HSK
									</code></pre>

									<p>
										Now we can load the pseudo data <a href="https://arxiv.org/abs/2306.02796" target="_blank">
										from the MCTS paper</a>, run it through our preprocessing function, and convert it into
										a HuggingFace Dataset object. Here, I am using 500,000 sentence pairs, with an 80/20
										split for training and evaluation. As the dataset is already shuffled, we can load
										the train and eval set directly:
									</p>

									<pre class="language-py"><code class="language-py">
start = 0
stop = 500000
split = 400000

lines_complex = preprocess_data('zh_selected.ori', start, stop)
lines_simple = preprocess_data('zh_selected.sim', start, stop)

data_dict = {'complex': lines_complex[start:split], 'simple': lines_simple[start:split]}
ds_train = Dataset.from_dict(data_dict)
data_dict = {'complex': lines_complex[split:stop], 'simple': lines_simple[split:stop]}
ds_eval = Dataset.from_dict(data_dict)
									</code></pre>

									<p>
										Here's a look at the first training example in the Dataset. You can see
										that the HSK tokens were added to a few words, which are at HSK level 4:
									</p>

									<pre class="language"><code class="language">
ds_train['complex'][0]: '75公斤级比赛3个项目[4]的第一名均为中国选手李顺柱获得[4]。'
(The first place in all three events[4] of the 75 kg competition was won[4] by China's Li Shunzhu.)

ds_train['simple'][0]: '中国选手李顺珠在75公斤级三个项目[4]中获得[4]第一名。'
(China's Li Shunzhu won[4] first place in three events[4] in the 75 kg category.)
									</code></pre>

									<p>Now, tokenize the preprocessed data. Here I use a max_length of 128, with
										truncation for larger inputs and padding for smaller inputs:</p>

									<pre class="language-py"><code class="language-py">
# tokenize data
max_length = 128
def batch_tokenize_data(data):
    inputs = [example for example in data['complex']]
    targets = [example for example in data['simple']]

    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True)
    labels = tokenizer(targets, max_length=max_length, padding='max_length', truncation=True)

    model_inputs['labels'] = labels['input_ids']
    return model_inputs

tokenized_data_train = ds_train.map(batch_tokenize_data, batched=True)
tokenized_data_eval = ds_eval.map(batch_tokenize_data, batched=True)
									</code></pre>
									
									<p>
										Now, set up the trainer and begin model fine-tuning with trainer.train():
									</p>

									<pre class="language-py"><code class="language-py">
training_args = TrainingArguments(
    output_dir="./bart_simplification",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    eval_accumulation_steps = 1,
    num_train_epochs=5,
    weight_decay=0.01,
    save_total_limit=2,
    logging_steps=500
)

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_data_train,
    eval_dataset = tokenized_data_eval,
    tokenizer = tokenizer
)								

trainer.train()
									</code></pre>

									<p>
										I used Google Colab with a A100 GPU to perform the training step. Let's look at the loss during training:
									</p>

									<div class="container">
										<a class="image centered"><img src="../images/project_TS/bart_loss.png" style="width:60%;"/></a>
										<figcaption>Training and evaluation loss during fine-tuning</figcation>
									</div>

									<p>
										Five epochs looks like a good stopping point, as the validation loss is leveling off and I 
										want to avoid overfitting. Now that we have a trained model, let's try making a prediction on the same
										example I showed earlier:
									</p>

									<pre class="language-py"><code class="language-py">
from transformers import Text2TextGenerationPipeline
text2text_generator = Text2TextGenerationPipeline(model, tokenizer)
output = text2text_generator(sentence, max_length=128, do_sample=False)[0]['generated_text'].replace(" ","")
									</code></pre>
									<pre class="language"><code class="language">
Example ('complex'): '75公斤级比赛3个项目的第一名均为中国选手李顺柱获得。'
(The first place in all three events of the 75 kg competition was won by China's Li Shunzhu.)

Example ('simple'): '中国选手李顺珠在75公斤级三个项目[4]中获得[4]第一名。'
(China's Li Shunzhu won first place in three events in the 75 kg category.)	

Fine-tuned BART output: '75公斤级三个项目的第一名均由中国选手李顺柱获得。'
(The first place in all three events in the 75kg category went to China's Li Shunzhu.)
									</code></pre>

									<p>
										Not bad! It looks like the output of the fine-tuned model is at least fluent and can produce
										reasonable output. But it can be pretty difficult to tell if the output is sufficiently simple,
										or if it requires further simplification.
									</p>

									<h4>Conclusion</h4>
									<p>
										In Part 3, I've described a way to preprocess and tokenize complex/simple sentence pairs from
										a large dataset and apply them for fine-tuning on the Chinese BART model. The output looks fluent and
										captures the meaning of the original complex sentence, but how can we tell if the model is actually
										simplifying the sentence rather than simply rewording it? Let's tackle that in Part 4, where I evaluate
										the performance of this BART model as well as our LS pipeline described in Part 2.
									</p>
									
									<a href="../Projects/textsimplification_part4.html" style="font-size:20px; font-weight:bold;"><p style="text-align: center;">Continued in Part 4!</p></a>

									<h4>References</h4>
									<p style="font-size:12pt"><a id="lewis2019" style="color: inherit; text-decoration: none;">[1] Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training 
										for Natural Language Generation, Translation, and Comprehension," 
										<a href="https://doi.org/10.48550/arXiv.1910.13461" target="_blank">https://doi.org/10.48550/arXiv.1910.13461</a> (2019)</a> 
									</p>
									<p style="font-size:12pt"><a id="sun2023" style="color: inherit; text-decoration: none;">[2] Sun et al., "Teaching the Pre-trained Model to 
										Generate Simple Texts for Text Simplification," 
										<a href="https://doi.org/10.48550/arXiv.2305.12463" target="_blank">https://doi.org/10.48550/arXiv.2305.12463</a> (2023)</a> 
									</p>
									<p style="font-size:12pt"><a id="chong2024" style="color: inherit; text-decoration: none;">[3] Chong et al., "MCTS: A Multi-Reference 
										Chinese Text Simplification Dataset," 
										<a href="https://doi.org/10.48550/arXiv.2306.02796" target="_blank">https://doi.org/10.48550/arXiv.2306.02796</a> (2024)</a> 
									</p>
									<p style="font-size:12pt"><a id="shao2021" style="color: inherit; text-decoration: none;">[4] Shao et al., "CPT: A Pre-Trained Unbalanced
										 Transformer for Both Chinese Language Understanding and Generation," 
										<a href="https://doi.org/10.48550/arXiv.2109.05729" target="_blank">https://doi.org/10.48550/arXiv.2109.05729</a> (2021)</a> 
									</p>
								</article>

						</div>
					</div>
				<!-- Features -->
				<div id="features-wrapper">
					<div class="container">
						<div class="row">
						</div>
					</div>
				</div>
				</div>

				

<!-- Footer -->
<div id="footer-wrapper">
	<footer id="footer" class="container">
		<div class="row">
			<div class="col-12 col-12-small">
				<div id="contact">
					<h3>Contact</h3>
					<p style="font-size: 18px;">
						<a href="https://www.linkedin.com/in/brian-johnson-a36651ab" target="_blank">Message me on LinkedIn</a><br>
						<a href="mailto: johnsonrobotics24@gmail.com"> johnsonrobotics24@gmail.com</a><br>
						+1-708-621-1336</p>
					<a href="../resume_brian_johnson.pdf" download target="_blank" rel="noopener noreferrer" class="button icon solid fa-download">Download resume</a>
				</div>
			</div>
			
		</div>
		<div class="row">
			<div class="col-12">
				<div id="copyright">
					<ul class="menu">
						<li>Brian K. Johnson 2024</li><li>Designed from <a href="http://html5up.net" target="_blank" rel="noopener noreferrer">HTML5 UP</a></li>
					</ul>
				</div>
			</div>
		</div>
	</footer>
				</div>

		</div>

		<!-- Scripts -->

			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.dropotron.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>
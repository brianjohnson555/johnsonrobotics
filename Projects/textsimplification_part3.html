<!DOCTYPE HTML>
<!--
	Brian Johnson Robotics
	Adapted from a template from html5up.net | @ajlkn (html5up.net/license)
-->
<html>
	<head>
		<title>Brian Johnson Robotics</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<link rel="stylesheet" href="../assets/css/prism.css" />
		<link rel="shortcut icon" type="image/x-icon" href="../favicon.ico" />
	</head>
	<body class="is-preload no-sidebar">
		<script src="../assets/js/prism.js"></script>
		<div id="page-wrapper">

			<!-- Header -->
			<div id="header-wrapper">
				<header id="header" class="container">

					<!-- Logo -->
						<div id="logo">
							<h1><a href="../index.html">Brian Johnson, Ph.D.</a></h1>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="../index.html">Home</a></li>
								<li class="current"><a href="../projects.html">Projects</a></li>
								<li><a href="../publications.html">Publications</a></li>
								<li><a href="../resume.html">Resume and Contact</a></li>
							</ul>
						</nav>

				</header>
			</div>

			<!-- Main -->
				<div id="main-wrapper">
					<div class="container blog">
						<div id="content">

							<!-- Content -->
								<article>
									<div class="container">
											<a class="image featured"><img src="../images/stock_chinese.jpg" style="display:block; height:40%; width:30%; position:relative; margin-left: auto; margin-right: auto;"/></a>
									</div>
									
									
									<h2>Chinese Text Simplification for Reading Comprehension (Part 3)</h2>

									<ul style="text-align: center; font-size:small">
										<li class="button alt"><a href="../Projects/textsimplification.html" style="color:inherit; text-decoration: none">Part 1: <br> Overview</a></li>
										<li class="button alt"><a href="../Projects/textsimplification_part2.html" style="color:inherit; text-decoration: none">Part 2: <br> Lexical simplification</a></li>
										<li class="button alt2"><a href="../Projects/textsimplification_part3.html" style="color:inherit; text-decoration: none">Part 3: <br> BART fine-tuning</a></li>
										<li class="button alt"><a href="../Projects/textsimplification_part4.html" style="color:inherit; text-decoration: none">Part 4: <br> Performance/analysis</a></li>
									</ul>
										

									<h3>Part 3 Summary</h3>
									<p><i>I describe my process of fine-tuning and applying a Chinese language BART model to 
										achieve generalized text simplification.
									</i></p>

									<h3>Fine-tuning a BART model for sentence-level text simplification</h3>
									<p>
										<b>Bidirectional and Auto-Regressive Transformer (BART)</b> is a sequence-to-sequence model proposed by Lewis et al. <a href="#lewis2019">[1]</a> which uses 
										an encoder and decoder for NLP tasks. For some in-depth explanations, check out these
										resources for <a href="https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b" target="_blank">sequence-to-sequence models</a>, 
										<a href="https://huggingface.co/blog/bert-101" target="_blank">BERT</a>, and 
										<a href="https://www.analyticsvidhya.com/blog/2024/11/bart-model/" target="_blank">BART</a>.
										I will be fine-tuning an existing BART model for a sequence generation task: generating
										a simplified Chinese sentence based on a complex sentence input!
									</p>

									<div class="container">
										<a class="image centered"><img src="../images/project_TS/bart_lewis_2019.PNG" style="width:60%;"/></a>
										<figcaption>BART model as explained in Lewis et al. <a href="#lewis2019">[1]</a></figcation>
									</div>

									<p>
										Fine-tuning BART for text simplification tasks has been demonstrated in English <a href="#sun2023">[2]</a> as well as
										Chinese <a href="#chong2023">[3]</a> and has shown performance better than large language models like GPT 3.5
										when properly fine-tuned. I will be using a BART model trained on Chinese language data, <b>CPT/Chinese BART</b>,
										available on HuggingFace: <a href="https://huggingface.co/fnlp/bart-base-chinese", target="_blank">https://huggingface.co/fnlp/bart-base-chinese</a>.
										The model was implemented from the architecture described in Shao et al. <a href="#shao2021">[4]</a>
									</p>

									<p>Chinese BART is trained from masking sequences to predict the masked word; it is not trained for text simplification 
										and will need to be fine-tuned. Let's get into how to do that!
									</p>

									<h4>Data for fine-tuning</h4>
									<p>
										The concept of fine-tuning is simple enough -- you use the pre-trained model with its preexisting model weights and biases,
										and continue training the model from its current state using task-specific data. In this case, I want to train Chinese BART
										with text simplification parallel data - a complex sentence input, with a simple sentence to be predicted as the output.
									</p>
									<p>
										As I discussed in <a href="../Projects/textsimplification.html">part 1</a> of this project, the 
										MCTS paper <a href="#chong2024">[3]</a> provides a dataset of 691,474 pseudo-simplified sentence pairs, 
										generated by machine-translating complex Chinese sentences from the People's Daily Corpus to English, 
										applying English TS tools, and retranslating the simplified sentences back to Chinese.  								
									</p>
									<p>
										Since this dataset is "pseudo" data coming from machine translation, the fluency of the output is unfortunately
										not guaranteed. However, at the time of writing there is a significant lack of datasets for Chinese language TS
										compared to other languages like English, so the MCTS dataset is one of the best options available.
									</p>

									<h4>Preprocessing and tokenizing data</h4>
									<p>Since our BART model is prepared on HuggingFace, we can use the HuggingFace transformers
										package and framework for our fine-tuning. Let's start by preparing our data. An important feature
										to note in Chinese BART is that Chinese text does not contain spaces separating words, but the BART
										tokenizer is expecting this -- so let's add spaces between words in each sentence with jieba:
									</p>

									<!--TODO: add discussion of HSK distribution here-->

									<pre class="language-py"><code class="language-py">
import numpy as np
import jieba
import pickle
from transformers import TrainingArguments, Trainer
from datasets import load_dataset
from datasets import Dataset
from transformers import BertTokenizer, BartForConditionalGeneration
tokenizer = BertTokenizer.from_pretrained("fnlp/bart-base-chinese")
model = BartForConditionalGeneration.from_pretrained("fnlp/bart-base-chinese")

def pre_tokenize(sentence):
    return " ".join(jieba.lcut(sentence)) # add spaces to sentence for tokenizer
	
def preprocess_data(filename: str, start: int, stop: int):
    with open(filename, encoding="utf8") as f:
        lines_orig = f.read().splitlines()
    return [pre_tokenize(line) for line in lines_orig]
									</code></pre>

									<p><i>
										Aside: in a previous version of this pipeline, I also attempted to add HSK tokens for HSK levels
										4, 5, 6, 7-9 to each corresponding word in the text. The idea behind this is to train BART to substitute
										or remove higher-level HSK words with simpler words. However, the MCTS pseudo dataset does not contain 
										enough lexical simplification between the input and target sentences for this method to be effective.
									</i></p>

									<p>
										Now we can load the pseudo data <a href="https://arxiv.org/abs/2306.02796" target="_blank">
										from the MCTS paper</a>, run it through our preprocessing function, and convert it into
										a HuggingFace Dataset object. Here, I am using 500,000 sentence pairs, with an 80/20
										split for training and evaluation. As the dataset is already shuffled, we can load
										the train and eval set directly:
									</p>

									<pre class="language-py"><code class="language-py">
start = 0
stop = 500000
split = 450000

lines_complex = preprocess_data('zh_selected.ori', start, stop)
lines_simple = preprocess_data('zh_selected.sim', start, stop)

data_dict = {'complex': lines_complex[start:split], 'simple': lines_simple[start:split]}
ds_train = Dataset.from_dict(data_dict)
data_dict = {'complex': lines_complex[split:stop], 'simple': lines_simple[split:stop]}
ds_eval = Dataset.from_dict(data_dict)
									</code></pre>

									<p>
										Here's a look at the first training example in the Dataset. You can see
										that the HSK tokens were added to a few words, which are at HSK level 4:
									</p>

									<pre class="language"><code class="language">
ds_train['complex'][0]: '75 公斤 级 比赛 3 个 项目 的 第一名 均 为 中国 选手 李顺柱 获得 。'
(The first place in all three events of the 75 kg competition was won by China's Li Shunzhu.)

ds_train['simple'][0]: '中国 选手 李顺珠 在 75 公斤 级 三 个 项目 中 获得 第一名。'
(China's Li Shunzhu won first place in three events in the 75 kg category.)
									</code></pre>

									<p>Now, tokenize the preprocessed data. Here I use a max_length of 128, with
										truncation for larger inputs and padding for smaller inputs:</p>

									<pre class="language-py"><code class="language-py">
# tokenize data
max_length = 128
def batch_tokenize_data(data):
    inputs = [example for example in data['complex']]
    targets = [example for example in data['simple']]

    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True)
    labels = tokenizer(targets, max_length=max_length, padding='max_length', truncation=True)

    model_inputs['labels'] = labels['input_ids']
    return model_inputs

tokenized_data_train = ds_train.map(batch_tokenize_data, batched=True)
tokenized_data_eval = ds_eval.map(batch_tokenize_data, batched=True)
									</code></pre>
									<h4>Hyperparameter search using Optuna</h4>
									<p>
										To fine-tune this BART model, I have to make some decisions about the training parameters
										like batch size, learning rate, etc. The best way to determine optimal hyperparameters is to
										use optimized tuners like Optuna, which can automatically search over your defined 
										hyperparameter space. Let's perform tuning using the search space defined below:
									</p>

									<pre class="language-py"><code class="language-py">
# optuna fine-tuning
import optuna
import torch

def search_space(trial):
	# Define hyperparameter search space
	return {
		"learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-4, log=True),
		"per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [4, 8, 16]),
		"num_train_epochs": trial.suggest_int("num_train_epochs", 3, 10),
		"weight_decay": trial.suggest_float("weight_decay", 1e-5, 1e-2, log=True)
	}

# Define training arguments
def model_init():
	# Function to initialize the model for Trainer
	from transformers import BartForConditionalGeneration
	return BartForConditionalGeneration.from_pretrained("fnlp/bart-base-chinese", local_files_only=True)

def compute_metrics(eval_preds):
	prediction_tokens = eval_preds.predictions
	prediction_text = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in prediction_tokens]

	sari_score = sari.compute(
			predictions=prediction_text, # model output
			references=[[simple] for simple in tokenized_data_eval.select(range(5000))['simple']], # reference simple sentences
			sources=tokenized_data_eval.select(range(5000))['complex'] # complex sentence
		)
	return {"sari": sari_score["sari"]}

def preprocess_logits_for_metrics(logits, labels):
		# This is a memory leak workaround to avoid storing too many tensors that are not needed
		pred_ids = torch.argmax(logits[0], dim=-1)
		return pred_ids

training_args = TrainingArguments(
	output_dir = "./bart_hypersearch",
	eval_strategy = "epoch",
	per_device_eval_batch_size = 1,
	eval_accumulation_steps = 1, 
	logging_dir = "./logs",
	greater_is_better = True,
)

trainer = Trainer(
	model_init=model_init,
	args=training_args,
	train_dataset=tokenized_data_train.select(range(15000)),  # Use a subset for quick tuning
	eval_dataset=tokenized_data_eval.select(range(5000)),  
	processing_class = tokenizer,
	compute_metrics = compute_metrics,
	preprocess_logits_for_metrics=preprocess_logits_for_metrics, # preprocesses before sending to compute_metrics()
)

# Run Optuna hyperparameter search
best_trial = trainer.hyperparameter_search(
	direction="maximize",  # For maximizing performance (adjust as needed)
	hp_space=search_space,
	n_trials=10  # Number of trials
)

print(best_trial)
									</code></pre>

									<p>
										The hyperparameter search took an hour or two on an A100 GPU in Google Colab. Here were
										the results of the best trial:
									</p>

									<pre class="language-py"><code class="language-py">
BestRun(run_id='4', 
		objective=33.830690227020675, 
		hyperparameters={'learning_rate': 0.00010879820645860841, 
						'per_device_train_batch_size': 128, 
						'num_train_epochs': 9, 
						'weight_decay': 0.001750272177978174}, 
		run_summary=None)
									</code></pre>

									<h4>Fine-tuning BART with final parameters</h4>
									
									<p>
										Now, set up the trainer using the final params and begin model fine-tuning with trainer.train():
									</p>

									<pre class="language-py"><code class="language-py">
best_params = best_trial.hyperparameters
training_args = TrainingArguments(
	output_dir = "./bart_simplification",
	eval_strategy = "epoch",
	save_strategy = "epoch",
	learning_rate = best_params["learning_rate"],
	per_device_train_batch_size = best_params["per_device_train_batch_size"],
	num_train_epochs = best_params["num_train_epochs"],
	weight_decay = best_params["weight_decay"],
	eval_accumulation_steps = 1,
	per_device_eval_batch_size = 1,
	logging_dir = "./logs",
	logging_steps = 500,
	greater_is_better = True, # maximize SARI
)

trainer = Trainer(
	model = model,
	args = training_args,
	train_dataset = tokenized_data_train,
	eval_dataset = tokenized_data_eval,
	processing_class = tokenizer,
	compute_metrics = compute_metrics,
	preprocess_logits_for_metrics = preprocess_logits_for_metrics,
)

trainer.train()
									</code></pre>

									<p>
										I used the same A100 GPU to perform the training step. Let's look at the loss during training:
									</p>

									<div class="container">
										<a class="image centered"><img src="../images/project_TS/bart_loss.png" style="width:60%;"/></a>
										<figcaption>Training and evaluation loss during fine-tuning</figcation>
									</div>

									<p>
										I did not train the full 9 epochs for two reasons: validation loss leveling out (avoid overfitting),
										and high remaining runtime on the A100 GPU. The total training time was 5.5 hours.
										Regardless, with the final fine-tuned model, we can make a sample prediction on our dataset:
									</p>

									<pre class="language-py"><code class="language-py">
from transformers import Text2TextGenerationPipeline
text2text_generator = Text2TextGenerationPipeline(model, tokenizer)
output = text2text_generator(sentence, max_length=128, do_sample=False)[0]['generated_text'].replace(" ","") # remove spaces for detokenization
									</code></pre>
									<pre class="language"><code class="language">
Example ('complex'): '75公斤级比赛3个项目的第一名均为中国选手李顺柱获得。'
(The first place in all three events of the 75 kg competition was won by China's Li Shunzhu.)

Example ('simple'): '中国选手李顺珠在75公斤级三个项目中获得第一名。'
(China's Li Shunzhu won first place in three events in the 75 kg category.)	

Fine-tuned BART output: '75公斤级三个项目的第一名均由中国选手李顺柱获得。'
(The first place in all three events in the 75kg category went to China's Li Shunzhu.)
									</code></pre>

									<p>
										Not bad! It looks like the output of the fine-tuned model is at least fluent and can produce
										reasonable output. But it can be pretty difficult to tell if the output is sufficiently simple,
										or if it requires further simplification.
									</p>

									<h4>Conclusion</h4>
									<p>
										In Part 3, I've described a way to preprocess and tokenize complex/simple sentence pairs from
										a large dataset and apply them for fine-tuning on the Chinese BART model. The output looks fluent and
										captures the meaning of the original complex sentence, but how can we tell if the model is actually
										simplifying the sentence rather than simply rewording it? Let's tackle that in Part 4, where I evaluate
										the performance of this BART model as well as our LS pipeline described in Part 2.
									</p>
									
									<a href="../Projects/textsimplification_part4.html" style="font-size:20px; font-weight:bold;"><p style="text-align: center;">Continued in Part 4!</p></a>

									<h4>References</h4>
									<p style="font-size:12pt"><a id="lewis2019" style="color: inherit; text-decoration: none;">[1] Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training 
										for Natural Language Generation, Translation, and Comprehension," 
										<a href="https://doi.org/10.48550/arXiv.1910.13461" target="_blank">https://doi.org/10.48550/arXiv.1910.13461</a> (2019)</a> 
									</p>
									<p style="font-size:12pt"><a id="sun2023" style="color: inherit; text-decoration: none;">[2] Sun et al., "Teaching the Pre-trained Model to 
										Generate Simple Texts for Text Simplification," 
										<a href="https://doi.org/10.48550/arXiv.2305.12463" target="_blank">https://doi.org/10.48550/arXiv.2305.12463</a> (2023)</a> 
									</p>
									<p style="font-size:12pt"><a id="chong2024" style="color: inherit; text-decoration: none;">[3] Chong et al., "MCTS: A Multi-Reference 
										Chinese Text Simplification Dataset," 
										<a href="https://doi.org/10.48550/arXiv.2306.02796" target="_blank">https://doi.org/10.48550/arXiv.2306.02796</a> (2024)</a> 
									</p>
									<p style="font-size:12pt"><a id="shao2021" style="color: inherit; text-decoration: none;">[4] Shao et al., "CPT: A Pre-Trained Unbalanced
										 Transformer for Both Chinese Language Understanding and Generation," 
										<a href="https://doi.org/10.48550/arXiv.2109.05729" target="_blank">https://doi.org/10.48550/arXiv.2109.05729</a> (2021)</a> 
									</p>
								</article>

						</div>
					</div>
				<!-- Features -->
				<div id="features-wrapper">
					<div class="container">
						<div class="row">
						</div>
					</div>
				</div>
				</div>

				

<!-- Footer -->
<div id="footer-wrapper">
	<footer id="footer" class="container">
		<div class="row">
			<div class="col-12 col-12-small">
				<div id="contact">
					<h3>Contact</h3>
					<p style="font-size: 18px;">
						<a href="https://www.linkedin.com/in/brian-johnson-a36651ab" target="_blank">Message me on LinkedIn</a><br>
						<a href="mailto: johnsonrobotics24@gmail.com"> johnsonrobotics24@gmail.com</a><br>
						+1-708-621-1336</p>
					<a href="../resume_brian_johnson.pdf" download target="_blank" rel="noopener noreferrer" class="button icon solid fa-download">Download resume</a>
				</div>
			</div>
			
		</div>
		<div class="row">
			<div class="col-12">
				<div id="copyright">
					<ul class="menu">
						<li>Brian K. Johnson 2024</li><li>Designed from <a href="http://html5up.net" target="_blank" rel="noopener noreferrer">HTML5 UP</a></li>
					</ul>
				</div>
			</div>
		</div>
	</footer>
				</div>

		</div>

		<!-- Scripts -->

			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.dropotron.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>